# YUANDI ERP - ë°±ì—… ë° ë³µêµ¬ ì „ëµ

## ğŸ“‹ ëª©ì°¨
1. [ë°±ì—… ì•„í‚¤í…ì²˜](#ë°±ì—…-ì•„í‚¤í…ì²˜)
2. [ë°±ì—… êµ¬í˜„](#ë°±ì—…-êµ¬í˜„)
3. [ë³µêµ¬ ì ˆì°¨](#ë³µêµ¬-ì ˆì°¨)
4. [ì¬í•´ ë³µêµ¬ ê³„íš](#ì¬í•´-ë³µêµ¬-ê³„íš)
5. [í…ŒìŠ¤íŠ¸ ë° ê²€ì¦](#í…ŒìŠ¤íŠ¸-ë°-ê²€ì¦)

---

## ğŸ—ï¸ ë°±ì—… ì•„í‚¤í…ì²˜

### 3-2-1 ë°±ì—… ì „ëµ
- **3ê°œì˜ ë°±ì—… ë³µì‚¬ë³¸** ìœ ì§€
- **2ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë¯¸ë””ì–´** ì‚¬ìš©
- **1ê°œì˜ ì˜¤í”„ì‚¬ì´íŠ¸ ë°±ì—…** ë³´ê´€

### ë°±ì—… êµ¬ì„± ìš”ì†Œ

```mermaid
graph TD
    A[Production Database] --> B[Primary Backup]
    A --> C[Secondary Backup]
    B --> D[AWS S3 Seoul]
    C --> E[AWS S3 Tokyo]
    A --> F[Real-time Replication]
    F --> G[Standby Database]
```

### ë°±ì—… ë ˆë²¨ ë° ì£¼ê¸°

| ë°±ì—… ìœ í˜• | ì£¼ê¸° | ë³´ê´€ ê¸°ê°„ | ì €ì¥ ìœ„ì¹˜ | ìë™í™” |
|----------|------|-----------|----------|--------|
| ì „ì²´ ë°±ì—… | ì¼ê°„ | 30ì¼ | S3 + ë¡œì»¬ | âœ… |
| ì¦ë¶„ ë°±ì—… | 6ì‹œê°„ | 7ì¼ | Supabase | âœ… |
| íŠ¸ëœì­ì…˜ ë¡œê·¸ | ì‹¤ì‹œê°„ | 14ì¼ | WAL Archive | âœ… |
| ìŠ¤ëƒ…ìƒ· | ì£¼ê°„ | 90ì¼ | Cross-region | âœ… |
| ì•„ì¹´ì´ë¸Œ | ì›”ê°„ | 1ë…„ | Glacier | âœ… |

---

## ğŸ’¾ ë°±ì—… êµ¬í˜„

### 1. ìë™ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/backup/automated-backup.sh

set -e

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source /etc/yuandi/backup.env

# ë³€ìˆ˜ ì„¤ì •
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_ROOT="/backup"
BACKUP_DIR="$BACKUP_ROOT/$DATE"
LOG_FILE="$BACKUP_ROOT/logs/backup_$DATE.log"

# í•¨ìˆ˜ ì •ì˜
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

notify() {
    # Slack ë˜ëŠ” ì´ë©”ì¼ ì•Œë¦¼
    curl -X POST $SLACK_WEBHOOK -d "{\"text\": \"$1\"}"
}

# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$BACKUP_DIR"
mkdir -p "$BACKUP_ROOT/logs"

log "========== ë°±ì—… ì‹œì‘ =========="

# 1. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
backup_database() {
    log "ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹œì‘..."
    
    # PostgreSQL ë°±ì—… (Supabase)
    PGPASSWORD=$DB_PASSWORD pg_dump \
        -h $DB_HOST \
        -U $DB_USER \
        -d $DB_NAME \
        -f "$BACKUP_DIR/database_$DATE.sql" \
        --verbose \
        --no-owner \
        --no-privileges \
        --clean \
        --if-exists
    
    # ì••ì¶•
    gzip "$BACKUP_DIR/database_$DATE.sql"
    
    log "ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: database_$DATE.sql.gz"
}

# 2. íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—…
backup_files() {
    log "íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ì‹œì‘..."
    
    # ì—…ë¡œë“œëœ íŒŒì¼ ë°±ì—…
    tar -czf "$BACKUP_DIR/uploads_$DATE.tar.gz" \
        -C /app/public \
        uploads/
    
    # ì„¤ì • íŒŒì¼ ë°±ì—…
    tar -czf "$BACKUP_DIR/configs_$DATE.tar.gz" \
        -C /app \
        .env.production \
        vercel.json \
        next.config.js
    
    log "íŒŒì¼ ì‹œìŠ¤í…œ ë°±ì—… ì™„ë£Œ"
}

# 3. Supabase Storage ë°±ì—…
backup_storage() {
    log "Supabase Storage ë°±ì—… ì‹œì‘..."
    
    # Storage ë²„í‚· ë™ê¸°í™”
    npx supabase storage download \
        --bucket products \
        --local-path "$BACKUP_DIR/storage/products"
    
    npx supabase storage download \
        --bucket documents \
        --local-path "$BACKUP_DIR/storage/documents"
    
    # ì••ì¶•
    tar -czf "$BACKUP_DIR/storage_$DATE.tar.gz" \
        -C "$BACKUP_DIR" \
        storage/
    
    rm -rf "$BACKUP_DIR/storage"
    
    log "Storage ë°±ì—… ì™„ë£Œ"
}

# 4. ë©”íƒ€ë°ì´í„° ë°±ì—…
backup_metadata() {
    log "ë©”íƒ€ë°ì´í„° ë°±ì—… ì‹œì‘..."
    
    cat > "$BACKUP_DIR/metadata.json" <<EOF
{
    "backup_date": "$DATE",
    "backup_type": "automated",
    "database_version": "$(psql -V | head -n1)",
    "app_version": "$(cat /app/package.json | jq -r .version)",
    "total_records": {
        "orders": $(psql -t -c "SELECT COUNT(*) FROM orders"),
        "products": $(psql -t -c "SELECT COUNT(*) FROM products"),
        "users": $(psql -t -c "SELECT COUNT(*) FROM profiles")
    }
}
EOF
    
    log "ë©”íƒ€ë°ì´í„° ë°±ì—… ì™„ë£Œ"
}

# 5. S3 ì—…ë¡œë“œ
upload_to_s3() {
    log "S3 ì—…ë¡œë“œ ì‹œì‘..."
    
    # ë©”ì¸ ë¦¬ì „ ì—…ë¡œë“œ
    aws s3 sync "$BACKUP_DIR" \
        "s3://$S3_BUCKET/backups/$DATE/" \
        --storage-class STANDARD_IA \
        --sse AES256
    
    # í¬ë¡œìŠ¤ ë¦¬ì „ ë³µì œ
    aws s3 sync "$BACKUP_DIR" \
        "s3://$S3_BUCKET_DR/backups/$DATE/" \
        --region ap-northeast-1 \
        --storage-class STANDARD_IA \
        --sse AES256
    
    log "S3 ì—…ë¡œë“œ ì™„ë£Œ"
}

# 6. ë°±ì—… ê²€ì¦
verify_backup() {
    log "ë°±ì—… ê²€ì¦ ì‹œì‘..."
    
    # ì²´í¬ì„¬ ìƒì„±
    find "$BACKUP_DIR" -type f -exec sha256sum {} \; > "$BACKUP_DIR/checksums.txt"
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    BACKUP_SIZE=$(du -sh "$BACKUP_DIR" | cut -f1)
    
    # ìµœì†Œ í¬ê¸° í™•ì¸ (1MB)
    if [ $(du -b "$BACKUP_DIR" | cut -f1) -lt 1048576 ]; then
        log "ê²½ê³ : ë°±ì—… í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤"
        notify "âš ï¸ ë°±ì—… ê²½ê³ : ë°±ì—… í¬ê¸° ì´ìƒ ($BACKUP_SIZE)"
        return 1
    fi
    
    log "ë°±ì—… ê²€ì¦ ì™„ë£Œ: $BACKUP_SIZE"
}

# 7. ì •ë¦¬ ì‘ì—…
cleanup() {
    log "ì •ë¦¬ ì‘ì—… ì‹œì‘..."
    
    # 30ì¼ ì´ìƒ ëœ ë¡œì»¬ ë°±ì—… ì‚­ì œ
    find "$BACKUP_ROOT" -maxdepth 1 -type d -mtime +30 -exec rm -rf {} \;
    
    # S3 ë¼ì´í”„ì‚¬ì´í´ ì •ì±… ì ìš© (Glacier ì „í™˜)
    aws s3api put-bucket-lifecycle-configuration \
        --bucket $S3_BUCKET \
        --lifecycle-configuration file://s3-lifecycle.json
    
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    rm -rf "$BACKUP_DIR"/*.tmp
    
    log "ì •ë¦¬ ì‘ì—… ì™„ë£Œ"
}

# ë©”ì¸ ì‹¤í–‰
main() {
    # ì—ëŸ¬ í•¸ë“¤ë§
    trap 'handle_error $? $LINENO' ERR
    
    backup_database
    backup_files
    backup_storage
    backup_metadata
    upload_to_s3
    verify_backup
    cleanup
    
    log "========== ë°±ì—… ì™„ë£Œ =========="
    notify "âœ… ë°±ì—… ì„±ê³µ: $DATE ($BACKUP_SIZE)"
}

handle_error() {
    local exit_code=$1
    local line_number=$2
    log "âŒ ì—ëŸ¬ ë°œìƒ: Line $line_number, Exit code: $exit_code"
    notify "âŒ ë°±ì—… ì‹¤íŒ¨: Line $line_number, Exit code: $exit_code"
    exit $exit_code
}

# ì‹¤í–‰
main
```

### 2. ì¦ë¶„ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/backup/incremental-backup.sh

# WAL ì•„ì¹´ì´ë¹™ ì„¤ì •
cat >> /etc/postgresql/postgresql.conf <<EOF
wal_level = replica
archive_mode = on
archive_command = 'aws s3 cp %p s3://yuandi-wal-archive/%f'
archive_timeout = 300
EOF

# ë² ì´ìŠ¤ ë°±ì—…
pg_basebackup \
    -h $DB_HOST \
    -U replicator \
    -D /backup/base \
    -Fp \
    -Xs \
    -P \
    -R
```

### 3. ì‹¤ì‹œê°„ ë³µì œ ì„¤ì •

```sql
-- Supabaseì—ì„œ ì‹¤í–‰
-- ì½ê¸° ì „ìš© ë³µì œë³¸ ìƒì„±
CREATE PUBLICATION yuandi_pub FOR ALL TABLES;

-- êµ¬ë…ì ì„¤ì •
CREATE SUBSCRIPTION yuandi_sub
    CONNECTION 'host=standby.db.yuandi.com dbname=yuandi'
    PUBLICATION yuandi_pub;
```

---

## ğŸ”„ ë³µêµ¬ ì ˆì°¨

### 1. ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤ë³„ RTO/RPO

| ì‹œë‚˜ë¦¬ì˜¤ | RTO | RPO | ë³µêµ¬ ë°©ë²• |
|----------|-----|-----|----------|
| ë‹¨ì¼ ë ˆì½”ë“œ ì‚­ì œ | 5ë¶„ | 0 | íŠ¸ëœì­ì…˜ ë¡œê·¸ |
| í…Œì´ë¸” ì†ìƒ | 30ë¶„ | 1ì‹œê°„ | ì¦ë¶„ ë°±ì—… |
| ë°ì´í„°ë² ì´ìŠ¤ ì¥ì•  | 1ì‹œê°„ | 1ì‹œê°„ | ì „ì²´ ë°±ì—… |
| ì „ì²´ ì‹œìŠ¤í…œ ì¥ì•  | 4ì‹œê°„ | 24ì‹œê°„ | DR ì‚¬ì´íŠ¸ |

### 2. ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬

```bash
#!/bin/bash
# scripts/recovery/database-restore.sh

# ë³€ìˆ˜ ì„¤ì •
BACKUP_DATE=$1
RESTORE_POINT=$2

if [ -z "$BACKUP_DATE" ]; then
    echo "Usage: $0 <backup_date> [restore_point]"
    exit 1
fi

# 1. ë°±ì—… íŒŒì¼ ë‹¤ìš´ë¡œë“œ
download_backup() {
    echo "ë°±ì—… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘..."
    aws s3 cp \
        "s3://$S3_BUCKET/backups/$BACKUP_DATE/database_$BACKUP_DATE.sql.gz" \
        /tmp/restore.sql.gz
    
    gunzip /tmp/restore.sql.gz
}

# 2. ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬
restore_database() {
    echo "ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì‹œì‘..."
    
    # ê¸°ì¡´ ì—°ê²° ì¢…ë£Œ
    psql -c "SELECT pg_terminate_backend(pid) 
             FROM pg_stat_activity 
             WHERE datname = '$DB_NAME' 
             AND pid <> pg_backend_pid();"
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±
    dropdb --if-exists $DB_NAME
    createdb $DB_NAME
    
    # ë°±ì—… ë³µì›
    psql $DB_NAME < /tmp/restore.sql
}

# 3. Point-in-Time Recovery
pitr_recovery() {
    if [ -n "$RESTORE_POINT" ]; then
        echo "PITR to $RESTORE_POINT..."
        
        cat > /var/lib/postgresql/recovery.conf <<EOF
restore_command = 'aws s3 cp s3://yuandi-wal-archive/%f %p'
recovery_target_time = '$RESTORE_POINT'
recovery_target_action = 'promote'
EOF
        
        pg_ctl restart
    fi
}

# 4. ê²€ì¦
verify_restore() {
    echo "ë³µêµ¬ ê²€ì¦ ì¤‘..."
    
    # ë°ì´í„° ìˆ˜ í™•ì¸
    ORDERS=$(psql -t -c "SELECT COUNT(*) FROM orders")
    PRODUCTS=$(psql -t -c "SELECT COUNT(*) FROM products")
    
    echo "ë³µêµ¬ëœ ë°ì´í„°:"
    echo "- ì£¼ë¬¸: $ORDERS"
    echo "- ìƒí’ˆ: $PRODUCTS"
    
    # ë¬´ê²°ì„± ê²€ì‚¬
    psql -c "SELECT * FROM verify_data_integrity();"
}

# ì‹¤í–‰
download_backup
restore_database
pitr_recovery
verify_restore

echo "âœ… ë³µêµ¬ ì™„ë£Œ"
```

### 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µêµ¬

```bash
#!/bin/bash
# scripts/recovery/app-restore.sh

# 1. ì½”ë“œ ë¡¤ë°±
rollback_code() {
    echo "ì½”ë“œ ë¡¤ë°±..."
    
    # Git íƒœê·¸ë¡œ ë¡¤ë°±
    git fetch --tags
    git checkout $RELEASE_TAG
    
    # ì˜ì¡´ì„± ì¬ì„¤ì¹˜
    npm ci --production
}

# 2. í™˜ê²½ ë³€ìˆ˜ ë³µêµ¬
restore_env() {
    echo "í™˜ê²½ ë³€ìˆ˜ ë³µêµ¬..."
    
    # S3ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë‹¤ìš´ë¡œë“œ
    aws s3 cp \
        "s3://$S3_BUCKET/configs/.env.production" \
        .env.production
    
    # Vercel í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
    vercel env pull
}

# 3. ì¬ë°°í¬
redeploy() {
    echo "ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ë°°í¬..."
    
    # ë¹Œë“œ
    npm run build
    
    # Vercel ë°°í¬
    vercel --prod --yes
}

# ì‹¤í–‰
rollback_code
restore_env
redeploy
```

---

## ğŸŒŠ ì¬í•´ ë³µêµ¬ ê³„íš (DRP)

### 1. ì¬í•´ ì‹œë‚˜ë¦¬ì˜¤

| ë ˆë²¨ | ì‹œë‚˜ë¦¬ì˜¤ | ì˜í–¥ ë²”ìœ„ | ëŒ€ì‘ ì „ëµ |
|------|----------|----------|----------|
| 1 | ì„œë²„ ì¥ì•  | ë‹¨ì¼ ì„œë²„ | ìë™ í˜ì¼ì˜¤ë²„ |
| 2 | ë°ì´í„°ì„¼í„° ì¥ì•  | ë‹¨ì¼ ë¦¬ì „ | ë¦¬ì „ ì „í™˜ |
| 3 | í´ë¼ìš°ë“œ ì œê³µì ì¥ì•  | ì „ì²´ ì„œë¹„ìŠ¤ | ë©€í‹° í´ë¼ìš°ë“œ |
| 4 | ìì—°ì¬í•´ | ë¬¼ë¦¬ì  ìœ„ì¹˜ | DR ì‚¬ì´íŠ¸ |

### 2. DR ì‚¬ì´íŠ¸ êµ¬ì„±

```yaml
# dr-site-config.yaml
primary_site:
  region: ap-northeast-2  # Seoul
  database: supabase-prod.com
  storage: s3://yuandi-prod
  cdn: cloudflare

dr_site:
  region: ap-northeast-1  # Tokyo
  database: supabase-dr.com
  storage: s3://yuandi-dr
  cdn: cloudflare-dr
  
failover:
  auto_failover: true
  health_check_interval: 60s
  failover_threshold: 3
  dns_ttl: 60
```

### 3. í˜ì¼ì˜¤ë²„ ì ˆì°¨

```bash
#!/bin/bash
# scripts/dr/failover.sh

# 1. í—¬ìŠ¤ ì²´í¬
check_primary() {
    curl -f https://api.yuandi.com/health || return 1
}

# 2. DNS ì „í™˜
switch_dns() {
    # Cloudflare APIë¥¼ í†µí•œ DNS ì—…ë°ì´íŠ¸
    curl -X PATCH \
        "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID" \
        -H "Authorization: Bearer $CF_TOKEN" \
        -H "Content-Type: application/json" \
        --data "{\"content\": \"$DR_IP\"}"
}

# 3. ë°ì´í„°ë² ì´ìŠ¤ í”„ë¡œëª¨ì…˜
promote_standby() {
    # Standbyë¥¼ Primaryë¡œ ìŠ¹ê²©
    pg_ctl promote -D /var/lib/postgresql/data
}

# 4. ì•Œë¦¼ ë°œì†¡
notify_team() {
    # Slack ì•Œë¦¼
    curl -X POST $SLACK_WEBHOOK \
        -d "{\"text\": \"ğŸš¨ DR í™œì„±í™”: Primary ì‚¬ì´íŠ¸ ì¥ì• \"}"
    
    # ì´ë©”ì¼ ì•Œë¦¼
    echo "DR í™œì„±í™”ë¨" | mail -s "YUANDI DR Alert" ops@yuandi.com
}

# ë©”ì¸ ì‹¤í–‰
if ! check_primary; then
    echo "Primary ì‚¬ì´íŠ¸ ì¥ì•  ê°ì§€"
    switch_dns
    promote_standby
    notify_team
    echo "âœ… DR ì‚¬ì´íŠ¸ í™œì„±í™” ì™„ë£Œ"
fi
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 1. ë°±ì—… í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì¼ê°„ í…ŒìŠ¤íŠ¸
- [ ] ë°±ì—… ì‘ì—… ì„±ê³µ ì—¬ë¶€
- [ ] ë°±ì—… íŒŒì¼ í¬ê¸° ê²€ì¦
- [ ] ì²´í¬ì„¬ í™•ì¸
- [ ] S3 ì—…ë¡œë“œ í™•ì¸

#### ì£¼ê°„ í…ŒìŠ¤íŠ¸
- [ ] ìƒ˜í”Œ ë³µêµ¬ í…ŒìŠ¤íŠ¸
- [ ] ë³µêµ¬ ì‹œê°„ ì¸¡ì •
- [ ] ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
- [ ] ì• í”Œë¦¬ì¼€ì´ì…˜ ë™ì‘ í™•ì¸

#### ì›”ê°„ í…ŒìŠ¤íŠ¸
- [ ] ì „ì²´ ë³µêµ¬ í›ˆë ¨
- [ ] DR ì‚¬ì´íŠ¸ ì „í™˜ í…ŒìŠ¤íŠ¸
- [ ] RTO/RPO ëª©í‘œ ë‹¬ì„± í™•ì¸
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸

### 2. ìë™í™”ëœ ë°±ì—… ê²€ì¦

```typescript
// scripts/verify-backup.ts
import { createClient } from '@supabase/supabase-js'
import * as AWS from 'aws-sdk'

const s3 = new AWS.S3()

async function verifyBackup(backupDate: string) {
  const checks = []
  
  // 1. S3 ë°±ì—… íŒŒì¼ ì¡´ì¬ í™•ì¸
  const s3Files = await s3.listObjectsV2({
    Bucket: process.env.S3_BUCKET!,
    Prefix: `backups/${backupDate}/`
  }).promise()
  
  checks.push({
    name: 'S3 ë°±ì—… íŒŒì¼',
    status: s3Files.Contents?.length > 0,
    details: `${s3Files.Contents?.length} files found`
  })
  
  // 2. ë°±ì—… í¬ê¸° í™•ì¸
  const totalSize = s3Files.Contents?.reduce(
    (acc, file) => acc + (file.Size || 0), 
    0
  ) || 0
  
  checks.push({
    name: 'ë°±ì—… í¬ê¸°',
    status: totalSize > 1024 * 1024, // 1MB ì´ìƒ
    details: `${(totalSize / 1024 / 1024).toFixed(2)} MB`
  })
  
  // 3. ë©”íƒ€ë°ì´í„° í™•ì¸
  const metadata = await s3.getObject({
    Bucket: process.env.S3_BUCKET!,
    Key: `backups/${backupDate}/metadata.json`
  }).promise()
  
  checks.push({
    name: 'ë©”íƒ€ë°ì´í„°',
    status: !!metadata.Body,
    details: 'Metadata file exists'
  })
  
  // 4. ì²´í¬ì„¬ ê²€ì¦
  const checksums = await s3.getObject({
    Bucket: process.env.S3_BUCKET!,
    Key: `backups/${backupDate}/checksums.txt`
  }).promise()
  
  checks.push({
    name: 'ì²´í¬ì„¬',
    status: !!checksums.Body,
    details: 'Checksums verified'
  })
  
  // ê²°ê³¼ ì¶œë ¥
  console.table(checks)
  
  // ì•Œë¦¼ ë°œì†¡
  const failed = checks.filter(c => !c.status)
  if (failed.length > 0) {
    await sendAlert(`ë°±ì—… ê²€ì¦ ì‹¤íŒ¨: ${failed.map(f => f.name).join(', ')}`)
  }
  
  return checks.every(c => c.status)
}

// ì‹¤í–‰
verifyBackup(new Date().toISOString().split('T')[0])
```

### 3. ë³µêµ¬ ì‹œê°„ ì¸¡ì •

```bash
#!/bin/bash
# scripts/measure-rto.sh

START_TIME=$(date +%s)

# ë³µêµ¬ ì‹¤í–‰
./scripts/recovery/database-restore.sh $1

END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

echo "ë³µêµ¬ ì‹œê°„: $(($DURATION / 60))ë¶„ $(($DURATION % 60))ì´ˆ"

# RTO ëª©í‘œ í™•ì¸
if [ $DURATION -lt 3600 ]; then
    echo "âœ… RTO ëª©í‘œ ë‹¬ì„± (1ì‹œê°„ ì´ë‚´)"
else
    echo "âŒ RTO ëª©í‘œ ì´ˆê³¼"
fi
```

---

## ğŸ“Š ë°±ì—… ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```typescript
// components/backup-dashboard.tsx
export function BackupDashboard() {
  return (
    <div className="grid grid-cols-2 gap-4">
      <MetricCard
        title="ë§ˆì§€ë§‰ ë°±ì—…"
        value={lastBackup}
        status={getBackupStatus()}
      />
      <MetricCard
        title="ë°±ì—… í¬ê¸°"
        value={backupSize}
        trend={sizeTrend}
      />
      <MetricCard
        title="ë³µêµ¬ í…ŒìŠ¤íŠ¸"
        value={lastTest}
        nextTest={nextTestDate}
      />
      <MetricCard
        title="RTO/RPO"
        current={{ rto: '45ë¶„', rpo: '1ì‹œê°„' }}
        target={{ rto: '1ì‹œê°„', rpo: '1ì‹œê°„' }}
      />
    </div>
  )
}
```

---

## ğŸ“ ë¹„ìƒ ì—°ë½ë§

| ì—­í•  | ë‹´ë‹¹ì | ì—°ë½ì²˜ | ìš°ì„ ìˆœìœ„ |
|------|--------|--------|----------|
| DBA | ê¹€ì² ìˆ˜ | 010-1234-5678 | 1 |
| ì‹œìŠ¤í…œ ê´€ë¦¬ì | ì´ì˜í¬ | 010-2345-6789 | 1 |
| ê°œë°œíŒ€ì¥ | ë°•ë¯¼ìˆ˜ | 010-3456-7890 | 2 |
| AWS ì§€ì› | - | 1588-1234 | 3 |
| Supabase ì§€ì› | - | support@supabase.io | 3 |

---

ìµœì¢… ì—…ë°ì´íŠ¸: 2024ë…„ 8ì›”